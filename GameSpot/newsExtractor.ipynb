{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import openpyxl\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of last page avalaible\n",
    "def get_last_news_page() -> int:\n",
    "    url = \"https://www.gamespot.com/news/\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    btn = soup.find_all('a', class_='btn', recursive=True)\n",
    "    pages = []\n",
    "    for b in btn:\n",
    "        try:\n",
    "            p = int(b.text)\n",
    "            pages.append(p)\n",
    "        except:\n",
    "            continue\n",
    "    last_page = max(pages)\n",
    "    return last_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all news from first_page to the last_page. Returns array of dictionaries where each of them has info about 100 pages of news\n",
    "def get_gamespot_news(last_page: int, news_min: int, news_max: int, display: bool = False) -> dict:    \n",
    "    titles, links, dates, pages = [],[],[],[]\n",
    "    url = \"https://www.gamespot.com/news/\"\n",
    "    sum_news = 0\n",
    "    for i in range(last_page):\n",
    "        try:\n",
    "            response = requests.get(url + \"?page=\" + str(last_page - i))\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            news = soup.find_all('div', class_=\"card-item__content\")\n",
    "            news = news[::-1]\n",
    "            if (len(news) + sum_news) < news_min:\n",
    "                sum_news += len(news)\n",
    "                continue\n",
    "            elif (len(news) + 120 + sum_news) < sum_news:\n",
    "                sum_news += len(news) + 120\n",
    "                i += 2\n",
    "                continue\n",
    "            for j in range(len(news)):\n",
    "                if news_min <= sum_news + 1 <= news_max:\n",
    "                    a = news[j].find('a', class_='card-item__link')\n",
    "                    title = a.text\n",
    "                    link = \"https://www.gamespot.com\" + str(a.get('href'))\n",
    "                    date = news[j].find('time').get('datetime')\n",
    "                    titles.append(title)\n",
    "                    links.append(link)\n",
    "                    dates.append(date)\n",
    "                    pages.append(last_page - i)\n",
    "                    if display == True:\n",
    "                        if (sum_news - news_min + 1) % 100 == 0 and (sum_news - news_min + 1) != 0:\n",
    "                            loaded = (sum_news - news_min + 1)\n",
    "                            percent = round((100 * loaded)/(news_max - news_min + 1), 2)\n",
    "                            print(loaded, \"news loaded ( \" + str(percent) + \"% )\")\n",
    "                if sum_news > news_max:\n",
    "                    final_dict = {}\n",
    "                    final_dict['titles'] = titles\n",
    "                    final_dict['links'] = links\n",
    "                    final_dict['dates'] = dates\n",
    "                    return final_dict\n",
    "                \n",
    "                sum_news += 1\n",
    "        except:\n",
    "            print(\"Error occured\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 news loaded ( 20.0% )\n",
      "200 news loaded ( 40.0% )\n",
      "300 news loaded ( 60.0% )\n",
      "400 news loaded ( 80.0% )\n",
      "news 1-500 saved\n",
      "\n",
      "100 news loaded ( 20.0% )\n",
      "200 news loaded ( 40.0% )\n",
      "300 news loaded ( 60.0% )\n",
      "400 news loaded ( 80.0% )\n",
      "news 501-1000 saved\n",
      "\n",
      "100 news loaded ( 20.0% )\n",
      "200 news loaded ( 40.0% )\n",
      "300 news loaded ( 60.0% )\n",
      "400 news loaded ( 80.0% )\n",
      "news 1001-1500 saved\n",
      "\n",
      "100 news loaded ( 20.0% )\n",
      "200 news loaded ( 40.0% )\n",
      "300 news loaded ( 60.0% )\n",
      "400 news loaded ( 80.0% )\n",
      "news 1501-2000 saved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "folder_name = datetime.now()\n",
    "folder_name = folder_name.strftime('%Y_%m_%d %H-%M-%S')\n",
    "os.makedirs(\"Data/\" + folder_name, exist_ok=True)\n",
    "n = 4\n",
    "from_value = 1\n",
    "to_value = 500\n",
    "last_page = get_last_news_page()\n",
    "for i in range(n):\n",
    "    l = from_value + (to_value * i)\n",
    "    r = to_value + (to_value * i)\n",
    "    dict_news = get_gamespot_news(last_page, l, r, True)\n",
    "    df = pd.DataFrame(dict_news)\n",
    "    df.to_csv(\"Data/\" + folder_name + \"/news\" + str(l) + '-' + str(r) + \".csv\", index=False)\n",
    "    print(\"news\", str(l) + '-' + str(r), \"saved\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_folder_name = \"2024_05_12 23-38-02\"\n",
    "os.makedirs(\"Data/Sheets\", exist_ok=True)\n",
    "file_names = os.listdir(\"Data/\" + target_folder_name)\n",
    "\n",
    "filenames_values = []\n",
    "csv_file_names = []\n",
    "for f in file_names:\n",
    "    pattern = r'news(\\d+)-(\\d+)\\.csv'\n",
    "    match = re.match(pattern, f)\n",
    "    if match:\n",
    "        value1 = int(match.group(1))\n",
    "        value2 = int(match.group(2))\n",
    "        filenames_values.append(value1)\n",
    "        filenames_values.append(value2)\n",
    "        csv_file_names.append(f)\n",
    "\n",
    "excel_path = \"Data/Sheets/NewsSheet\" + target_folder_name + \" \" + str(min(filenames_values)) + '-' + str(max(filenames_values)) + \".xlsx\"\n",
    "with pd.ExcelWriter(excel_path) as writer:\n",
    "\n",
    "    for f in csv_file_names:\n",
    "        df = pd.read_csv(\"Data/\" + target_folder_name + '/' + f)\n",
    "        name = f.replace(\".csv\", \"\")\n",
    "        df.to_excel(writer, sheet_name=name, index=False)\n",
    "        workbook = writer.book\n",
    "        worksheet = workbook[name] \n",
    "        \n",
    "        for row in worksheet.iter_rows():\n",
    "            for cell in row:\n",
    "                cell.alignment = openpyxl.styles.Alignment(wrap_text=True)\n",
    "        for col in worksheet.columns:\n",
    "            max_length = 0\n",
    "            column = col[0].column_letter\n",
    "            for cell in col:\n",
    "                try:\n",
    "                    if len(str(cell.value)) > max_length:\n",
    "                        max_length = len(cell.value)\n",
    "                except:\n",
    "                    pass\n",
    "            adjusted_width = (max_length + 2) * 1.2 \n",
    "            worksheet.column_dimensions[column].width = adjusted_width\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
