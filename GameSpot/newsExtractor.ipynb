{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of last page avalaible\n",
    "def get_last_news_page() -> int:\n",
    "    url = \"https://www.gamespot.com/news/\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    btn = soup.find_all('a', class_='btn', recursive=True)\n",
    "    pages = []\n",
    "    for b in btn:\n",
    "        try:\n",
    "            p = int(b.text)\n",
    "            pages.append(p)\n",
    "        except:\n",
    "            continue\n",
    "    last_page = max(pages)\n",
    "    return last_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all news from first_page to the last_page. Returns array of dictionaries where each of them has info about 100 pages of news\n",
    "def get_all_gamespot_news(last_page: int, first_page: int = 1) -> list[dict]:    \n",
    "    dict_array = []\n",
    "    for i in range(last_page):\n",
    "        # if i == 202:\n",
    "            # break\n",
    "        if i%100 == 0:\n",
    "            titles, links, dates, pages = [],[],[],[]\n",
    "            if i != 0:\n",
    "                final_dict = {}\n",
    "                final_dict['titles'] = titles\n",
    "                final_dict['links'] = links\n",
    "                final_dict['dates'] = dates\n",
    "                final_dict['pages'] = pages\n",
    "                dict_array.append(final_dict)\n",
    "        try:\n",
    "            response = requests.get(url + \"?page=\" + str(last_page - i))\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            news = soup.find_all('div', class_=\"card-item__content\")\n",
    "            for i in range(len(news)):\n",
    "                a = news[i].find('a', class_='card-item__link')\n",
    "                title = a.text\n",
    "                link = \"https://www.gamespot.com\" + str(a.get('href'))\n",
    "                date = news[i].find('time').get('datetime')\n",
    "                titles.append(title)\n",
    "                links.append(link)\n",
    "                dates.append(date)\n",
    "                pages.append(last_page - i)\n",
    "        except ConnectionError as e:\n",
    "            print(e)\n",
    "        if last_page - i == first_page:\n",
    "            break\n",
    "    return dict_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_array = get_all_gamespot_news(get_last_news_page())\n",
    "for i, d in enumerate(dict_array):\n",
    "    pages = dict_array[i]['pages']\n",
    "    del dict_array[i]['pages']\n",
    "    df = pd.DataFrame(d)\n",
    "    df.to_csv(\"Data/Result/pages\" + str(max(pages)) + \"-\" + str(min(pages)) + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
